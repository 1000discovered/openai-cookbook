{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Voice Translation of Audio files \n",
    "\n",
    "Ever wanted to translate a podcast into your native language? Translating and dubbing podcasts can make content more accessible to a global audience. This guide will walk you through the process of converting an English audio file into Hindi using OpenAI APIs. We will cover transcription of the audio, translation into target script, text-to-speech conversion into target language, and benchmarking to ensure quality and accuracy.\n",
    "\n",
    "A note on semantics used in this Cookbook regarding **Language** and **Script**. These words are generally used interchangeably, though it's important to understand the distinction given the task at hand. \n",
    "- **Language** refers to the spoken or written system of communication. For instance, Hindi and Marathi are different languages, but both use the Devanagari script. Similarly, English and French are different languages, but are written in Latin script. \n",
    "- **Script** refers to the set of characters or symbols used to write the language. For example, Serbian language traditionally written in Cyrillic Script, is also written in Latin script.\n",
    "\n",
    "\n",
    "In this cookbook we will walk through steps to dub an audio podcast from English to Hindi. Overall steps are:    \n",
    "\n",
    "1. **Transcribe** the audio file into text with Whisper \n",
    "2. **Translate** the English language text to Hindi in Devanagari script    \n",
    "3. **Text-to-speech** conversion of the Devanagari script into spoken Hindi language \n",
    "4. **Translation benchmarking** (BLEU or ROUGE) \n",
    "5. **Interpret and improve** scores by adjusting prompting parameters in steps 1-3 as needed "
   ],
   "id": "4265ef1326248608"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Step 1: Transcribe the audio file into text with Whisper\n",
    "\n",
    "Whisper is an open-source automatic speech recognition (ASR) system developed by OpenAI. It can transcribe audio files with high accuracy across multiple languages. \n",
    "\n",
    "\n",
    "![Text-to-speech](./images/Whisper.png)\n",
    "\n",
    "*[Learn more about Whisper here](https://platform.openai.com/docs/guides/speech-to-text) \n"
   ],
   "id": "d84f1356379946f6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-03T21:33:46.395533Z",
     "start_time": "2024-09-03T21:33:36.986122Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "audio_file = \"../gpt4o/data/keynote_recap.mp3\"\n",
    "\n",
    "audio_file= open(audio_file, \"rb\")\n",
    "transcription = client.audio.transcriptions.create(\n",
    "  model=\"whisper-1\",  \n",
    "  file=audio_file\n",
    ")\n",
    "transcription_english_first_pass = transcription.text\n",
    "print(transcription_english_first_pass)"
   ],
   "id": "7d10f34bb10ab45",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to our first-ever OpenAI Dev Day. Today, we are launching a new model, GPT-4 Turbo. GPT-4 Turbo supports up to 128,000 tokens of context. We have a new feature called JSON mode, which ensures that the model will respond with valid JSON. You can now call many functions at once, and it'll do better at following instructions in general. You want these models to be able to access better knowledge about the world. So do we. So we're launching retrieval in the platform. You can bring knowledge from outside documents or databases into whatever you're building. GPT-4 Turbo has knowledge about the world up to April of 2023, and we will continue to improve that over time. Dolly 3, GPT-4 Turbo with Vision, and the new Text-to-Speech model are all going into the API today. Today, we're launching a new program called Custom Models. With Custom Models, our researchers will work closely with the company to help them make a great custom model, especially for them and their use case using our tools, higher rate limits. We're doubling the tokens per minute for all of our established GPT-4 customers so that it's easier to do more, and you'll be able to request changes to further rate limits and quotas directly in your API account settings. And GPT-4 Turbo is considerably cheaper than GPT-4 by a factor of 3x for prompt tokens and 2x for completion tokens, starting today. We're thrilled to introduce GPTs. GPTs are tailored versions of chat GPT for a specific purpose. And because they combine instructions, expanded knowledge, and actions, they can be more helpful to you. They can work better in many contexts, and they can give you better control. We know that many people who want to build a GPT don't know how to code. We've made it so that you can program the GPT just by having a conversation. You can make private GPTs. You can share your creations publicly with a link for anyone to use. Or if you're on chat GPT Enterprise, you can make GPTs just for your company. And later this month, we're going to launch the GPT Store. So those are the GPTs, and we can't wait to see what you'll build. We're bringing the same concept to the API. The Assistance API includes persistent threads, so they don't have to figure out how to deal with long conversation history, built-in retrieval, code interpreter, a working Python interpreter in a sandbox environment, and of course, the improved function calling. As intelligence gets integrated everywhere, we will all have superpowers on demand. We're excited to see what you all will do with this technology, and to discover the new future that we're all going to architect together. We hope that you'll come back next year. What we launched today is going to look very quaint relative to what we're busy creating for you now. Thank you for all that you do. Thanks for coming here today.\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Note that the model transcribed \"Dall-e\" as \"Dolly\". This is a common issue with similar sounding words. \n",
    "\n",
    "There are two techniques to correct such transcription errors: \n",
    "\n",
    "#### Option 1: Provide the correct transcription in the `prompt` parameter. \n",
    "You could build a \"glossary\" of terms that model is likely to transcribe inaccurately given the context of the text. Over time, terms can be added to the glossary improving accuracy.  "
   ],
   "id": "3a8c242021ce3964"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-03T21:35:16.538050Z",
     "start_time": "2024-09-03T21:35:07.077238Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Path to the source audio file  \n",
    "audio_file = \"../gpt4o/data/keynote_recap.mp3\"\n",
    "\n",
    "# Transcription errors can be stored in a glossary of transcription errors data structure or variable \n",
    "glossary_of_transcription_errors = \"Dall-e\"\n",
    "\n",
    "# Invoke the model to get the transcription\n",
    "audio_file= open(audio_file, \"rb\")\n",
    "transcription = client.audio.transcriptions.create(\n",
    "  model=\"whisper-1\", \n",
    "    prompt=glossary_of_transcription_errors,   \n",
    "  file=audio_file\n",
    ")\n",
    "\n",
    "english_transcription_second_pass = transcription.text\n",
    "print(english_transcription_second_pass)"
   ],
   "id": "261ed580658bd77f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to our first ever OpenAI Dev Day. Today we are launching a new model, GPT-4 Turbo. GPT-4 Turbo supports up to 128,000 tokens of context. We have a new feature called JSON mode, which ensures that the model will respond with valid JSON. You can now call many functions at once, and it'll do better at following instructions in general. You want these models to be able to access better knowledge about the world. So do we. So we're launching retrieval in the platform. You can bring knowledge from outside documents or databases into whatever you're building. GPT-4 Turbo has knowledge about the world up to April of 2023, and we will continue to improve that over time. Dall-e 3, GPT-4 Turbo with Vision, and the new Text-to-Speech model are all going into the API today. Today we're launching a new program called Custom Models. With Custom Models, our researchers will work closely with the company to help them make a great custom model, especially for them and their use case using our tools, higher rate limits. We're doubling the tokens per minute for all of our established GPT-4 customers so that it's easier to do more, and you'll be able to request changes to further rate limits and quotas directly in your API account settings. And GPT-4 Turbo is considerably cheaper than GPT-4 by a factor of 3x for prompt tokens and 2x for completion tokens, starting today. We're thrilled to introduce GPTs. GPTs are tailored versions of chat GPT for a specific purpose. And because they combine instructions, expanded knowledge, and actions, they can be more helpful to you. They can work better in many contexts, and they can give you better control. We know that many people who want to build a GPT don't know how to code. We've made it so that you can program the GPT just by having a conversation. You can make private GPTs. You can share your creations publicly with a link for anyone to use. Or if you're on chat GPT Enterprise, you can make GPTs just for your company. And later this month, we're going to launch the GPT Store. So those are the GPTs, and we can't wait to see what you'll build. We're bringing the same concept to the API. The Assistance API includes persistent threads, so they don't have to figure out how to deal with long conversation history, built-in retrieval, code interpreter, a working Python interpreter in a sandbox environment, and of course, the improved function calling. As intelligence gets integrated everywhere, we will all have superpowers on demand. We're excited to see what you all will do with this technology, and to discover the new future that we're all going to architect together. We hope that you'll come back next year. What we launched today is going to look very quaint relative to what we're busy creating for you now. Thank you for all that you do. Thanks for coming here today.\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Now the model accurately transcribed word \"Dall-e\" as we provided the correct transcription in the prompt.\n",
    " \n",
    "\n",
    "#### Option 2: Use GPT model for post-processing output \n",
    "Another popular technique to improve the quality of output and correct spelling mistakes is to use GPT model for post-processing as described below."
   ],
   "id": "671ccdbfa5c5238d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-03T21:35:51.208951Z",
     "start_time": "2024-09-03T21:35:42.917070Z"
    }
   },
   "cell_type": "code",
   "source": [
    "system_prompt = f\"You are a helpful assistant. Your task is to correct any spelling discrepancies and grammar in the transcribed text. Only add necessary punctuation such as periods, commas, and capitalization, and use only the context provided. Some words that may be misspelled include: {glossary_of_transcription_errors}\" \n",
    "\n",
    "response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        temperature=0,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": system_prompt\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": transcription_english_first_pass\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "\n",
    "transcription_english_third_pass = response.choices[0].message.content"
   ],
   "id": "8eac35e5c4adb6ab",
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Step 2. Translate the English language text to Hindi in Devanagari script\n",
    "\n",
    "Next step is to transcribe the text from the given language, to the target language script. In this case we prompt the gpt-4o model to translate the text from English (written in Latin script) to Hindi (written in Devanagari script). \n",
    "\n",
    "For certain new words in a language, there may not be an available translation in the target language. In such cases, we prompt the model to retain the words in original language. We can also provide examples of words to keep in the original script (E.g., English). We can store these terms that we want to keep in source language as a glossary of terms. Which can be expanded as we translate the text. \n"
   ],
   "id": "5d294fa712197b74"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-03T21:37:03.945444Z",
     "start_time": "2024-09-03T21:36:54.935213Z"
    }
   },
   "cell_type": "code",
   "source": [
    "glossary_of_terms_to_keep_in_original_language = \"Some words to keep in English include - Turbo, OpenAI, token, GPT, Dall-e, Python\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-4o\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": f\"You are an assistant that translates text from English to Hindi. User will provide content for you to translate. You may keep certain words in English for which a direct translation doesn't exist. {glossary_of_terms_to_keep_in_original_language} \"},\n",
    "    {\"role\": \"user\", \"content\": transcription.text},\n",
    "  ]\n",
    ")\n",
    "\n",
    "hindi_transcription = response.choices[0].message.content\n",
    "\n",
    "print(hindi_transcription)"
   ],
   "id": "330baa9a6e55530c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "हमारे पहले OpenAI Dev Day में आपका स्वागत है। आज हम एक नया मॉडल, GPT-4 Turbo लॉन्च कर रहे हैं। GPT-4 Turbo 128,000 tokens तक का कॉन्टेक्स्ट सपोर्ट करता है। हमारे पास एक नई फीचर है जिसे JSON मोड कहा जाता है, जो सुनिश्चित करता है कि मॉडल वैध JSON के साथ प्रतिक्रिया करेगा। आप अब एक साथ कई फंक्शन्स को कॉल कर सकते हैं, और यह सामान्य रूप से निर्देशों का पालन करने में बेहतर करेगा। आप चाहते हैं कि ये मॉडल बेहतर ज्ञान को एक्सेस कर सके। हम भी यही चाहते हैं। इसलिए हम प्लेटफार्म में रिट्रीवल लॉन्च कर रहे हैं। आप बाहरी दस्तावेज़ों या डेटाबेस से ज्ञान को ला सकते हैं जो भी आप बना रहे हैं उसमें। GPT-4 Turbo के पास अप्रैल 2023 तक की दुनिया की जानकारी है, और हम इसे समय के साथ सुधारते रहेंगे। Dall-e 3, Vision के साथ GPT-4 Turbo, और नया Text-to-Speech मॉडल आज से API में उपलब्ध हैं।\n",
      "\n",
      "आज हम एक नया प्रोग्राम लॉन्च कर रहे हैं जिसे Custom Models कहा जाता है। Custom Models के साथ, हमारे शोधकर्ता कंपनी के साथ मिलकर काम करेंगे ताकि वे उनके उपयोग के मामले के लिए विशेष मॉडल बना सकें, हमारे औजारों का उपयोग करते हुए, अधिक रेट लिमिट्स के साथ। हम हमारे सभी स्थापित GPT-4 कस्टमर्स के लिए प्रति मिनट दोगुने tokens कर रहे हैं ताकि आप अधिक कर सकें, और आप अपनी API अकाउंट सेटिंग्स में सीधे रेट लिमिट्स और कोटा में परिवर्तन के लिए अनुरोध कर सकें। और GPT-4 Turbo काफी सस्ता है GPT-4 की तुलना में, prompt tokens के लिए 3x और completion tokens के लिए 2x प्राइस के साथ, आज से शुरू हो रहा है।\n",
      "\n",
      "हम गर्व के साथ GPTs का परिचय दे रहे हैं। GPTs विशेष उद्देश्य के लिए चैट GPT के अनुकूलित संस्करण हैं। और क्योंकि वे निर्देश, विस्तारित ज्ञान, और कार्रवाइयाँ मिलाते हैं, वे आपके लिए अधिक सहायक हो सकते हैं। वे कई संदर्भों में बेहतर काम कर सकते हैं, और आपको बेहतर नियंत्रण दे सकते हैं। हम जानते हैं कि कई लोग जो GPT बनाना चाहते हैं, कोड करना नहीं जानते। हमने इसे इस प्रकार बनाया है कि आप बातचीत के माध्यम से GPT को प्रोग्राम कर सकते हैं। आप निजी GPTs बना सकते हैं। आप अपनी क्रिएशन्स को सार्वजनिक रूप से लिंक के माध्यम से साझा कर सकते हैं ताकि कोई भी उनका उपयोग कर सके। या यदि आप Chat GPT Enterprise पर हैं, तो आप अपनी कंपनी के लिए GPTs बना सकते हैं। और इस महीने के अंत में, हम GPT Store लॉन्च करने जा रहे हैं। तो ये हैं GPTs, और हम देखना चाहते हैं कि आप क्या बनाएंगे।\n",
      "\n",
      "हम उसी कॉन्सेप्ट को API पर भी ला रहे हैं। Assistance API में persistent threads शामिल हैं, ताकि उन्हें लंबी बातचीत के इतिहास से निपटाने का तरीका खोजने की जरूरत न हो, बिल्ट-इन रिट्रीवल, कोड इंटरप्रेटर, एक कार्यशील Python इंटरप्रेटर सैंडबॉक्स वातावरण में, और निश्चित रूप से, सुधारित फंक्शन कॉलिंग शामिल है। जैसे जैसे बुद्धिमत्ता हर जगह एकीकृत हो रही है, हम सबके पास डिमांड पर सुपरपॉवर्स होंगे। हम इस तकनीक के साथ आप सभी क्या करेंगे, इसे देखने के लिए उत्साहित हैं, और उस नए भविष्य की खोज करने के लिए जिसे हम सब मिलकर बनाएंगे। हम आशा करते हैं कि आप अगले साल वापस आएंगे। आज हमने जो लॉन्च किया है, वह आने वाले समय में आपको बहुत छोटा लगेगा कि हम आपके लिए क्या बना रहे हैं। आपके द्वारा किए गए सभी कार्यों के लिए धन्यवाद। आज यहां आने के लिए धन्यवाद।\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The transcribed text is a combination of Hindi and English languages, represented in their respective scripts Devanagari and Latin. This will generate a more natural sounding voice with the righ purnounciation of the words.",
   "id": "d7d71ed42afacb8e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3. Text-to-speech conversion of the Devanagari script into spoken Hindi language\n",
    "\n",
    "OpenAI tts model can take the translated text as input and provide the spoken language output with native sounding Hindi, intermingled with native sounding English words, where appropriate.   \n",
    "\n",
    "![Text-to-speech](./images/Text-to-speech.png)\n",
    " \n",
    "\n",
    "*[Learn more about tts model here](https://platform.openai.com/docs/guides/text-to-speech)"
   ],
   "id": "67068cca4fe5211b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-03T21:38:18.356847Z",
     "start_time": "2024-09-03T21:37:59.004027Z"
    }
   },
   "cell_type": "code",
   "source": [
    "output_file_path = \"./sounds/output.mp3\"\n",
    "\n",
    "response = client.audio.speech.create(\n",
    "    model=\"tts-1\",\n",
    "    voice=\"alloy\",\n",
    "    input=hindi_transcription,\n",
    ")\n",
    "\n",
    "response.stream_to_file(output_file_path)"
   ],
   "id": "dc7922ea7f33b4d3",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jk/xl1ppdk94637355tx59m0kph0000gp/T/ipykernel_49771/3220431284.py:9: DeprecationWarning: Due to a bug, this method doesn't actually stream the response content, `.with_streaming_response.method()` should be used instead\n",
      "  response.stream_to_file(output_file_path)\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-03T21:20:57.144909Z",
     "start_time": "2024-09-03T21:17:27.422928Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from playsound import playsound\n",
    "\n",
    "playsound(output_file_path)"
   ],
   "id": "e5545d55bf8be52a",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Step 4. Translation benchmarking (e.g., BLEU or ROUGE) \n",
    "\n",
    "Assess the quality of the translated text by comparing it to a reference translation using evaluation metrics like BLEU and ROUGE. \n",
    "\n",
    "**BLEU (Bilingual Evaluation Understudy)**: Measures the overlap of n-grams between the candidate and reference translations. Scores range from 0 to 100, with higher scores indicating better quality.\n",
    "\n",
    "**ROUGE (Recall-Oriented Understudy for Gisting Evaluation)**: Commonly used for summarization evaluation. Measures the overlap of n-grams and longest common subsequence between candidate and reference texts.\n",
    "\n",
    "Typically, we need a reference translation (human-translated version) of the original text for accurate evaluation. However, for the purposes of this example, we will translate the text back from Hindi to English to get a metric on quality of translation. "
   ],
   "id": "9f25e38cbf8dbd26"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-03T21:40:18.462299Z",
     "start_time": "2024-09-03T21:40:10.605075Z"
    }
   },
   "cell_type": "code",
   "source": [
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-4o\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are an assistant that translates text from Hindi to English. User will provide content for you to translate.\"},\n",
    "    {\"role\": \"user\", \"content\": hindi_transcription},\n",
    "  ]\n",
    ")\n",
    "\n",
    "english_re_transcription = response.choices[0].message.content\n",
    "\n",
    "print(english_re_transcription)"
   ],
   "id": "d662ac09494780a3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to our first OpenAI Dev Day. Today, we are launching a new model, GPT-4 Turbo. GPT-4 Turbo supports context up to 128,000 tokens. We have a new feature called JSON mode, which ensures the model responds with valid JSON. You can now call multiple functions simultaneously, and it will generally perform better in following instructions. You want these models to access better knowledge. We want the same. So, we are launching retrieval in the platform. You can pull knowledge from external documents or databases into whatever you are building. GPT-4 Turbo has world information up to April 2023, and we will continue to improve it over time. Dall-e 3, Vision with GPT-4 Turbo, and the new Text-to-Speech model are available in the API starting today.\n",
      "\n",
      "Today, we are launching a new program called Custom Models. With Custom Models, our researchers will work with companies to create specific models for their use cases, using our tools, with higher rate limits. We are doubling the tokens per minute for all our established GPT-4 customers so you can do more, and you can request changes to rate limits and quota directly in your API account settings. And GPT-4 Turbo is significantly cheaper compared to GPT-4, with 3x the price for prompt tokens and 2x the price for completion tokens, starting today.\n",
      "\n",
      "We are proudly introducing GPTs. GPTs are customized versions of Chat GPT for special purposes. Since they combine instructions, extended knowledge, and actions, they can help you more. They can work better in many contexts and give you better control. We know many people who want to create GPTs don't know how to code. We've made it possible for you to program GPT through conversation. You can create private GPTs. You can share your creations publicly through a link so that anyone can use them. Or, if you are on Chat GPT Enterprise, you can create GPTs for your company. And at the end of this month, we are going to launch the GPT Store. So, these are the GPTs, and we want to see what you will build.\n",
      "\n",
      "We are bringing the same concept to the API. The Assistance API includes persistent threads, so they don't need to find a way to handle long conversation histories, built-in retrieval, a code interpreter, a working Python interpreter in a sandbox environment, and, of course, improved function calling. As intelligence is being integrated everywhere, we will all have superpowers on demand. We are excited to see what you will do with this technology and to explore the new future we will all create together. We hope you will come back next year. What we launched today will seem very small compared to what we are building for you in the future. Thank you for all the work you do. Thank you for being here today.\n"
     ]
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "fd673dbf860cdf62"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-03T21:38:55.518242Z",
     "start_time": "2024-09-03T21:38:55.464108Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sacrebleu\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# We'll use the original english transcription as the reference text \n",
    "reference_text = transcription_english_third_pass\n",
    "\n",
    "candidate_text = english_re_transcription\n",
    "\n",
    "# BLEU Score Evaluation\n",
    "bleu = sacrebleu.corpus_bleu([candidate_text], [[reference_text]])\n",
    "print(f\"BLEU Score: {bleu.score}\")\n",
    "\n",
    "# ROUGE Score Evaluation\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
    "scores = scorer.score(reference_text, candidate_text)\n",
    "print(f\"ROUGE-1 Score: {scores['rouge1'].fmeasure}\")\n",
    "print(f\"ROUGE-L Score: {scores['rougeL'].fmeasure}\")\n"
   ],
   "id": "b661a404001c660d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score: 49.23275305980889\n",
      "ROUGE-1 Score: 0.8346613545816735\n",
      "ROUGE-L Score: 0.7669322709163345\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Step 5. Interpret and improve scores by adjusting prompting parameters in steps 1-3 as needed\n",
    "\n",
    "In this example, both BLEU and ROUGE scores indicates we are close to an excellent outcome. \n",
    "\n",
    "**Interpreting BLEU Scores:** While there is no universally accepted scale, some interpretations suggest:\n",
    "\n",
    "0 to 10: Poor quality translation; significant errors and lack of fluency.\n",
    "\n",
    "10 to 20: Low quality; understandable in parts but contains many errors.\n",
    "\n",
    "20 to 30: Fair quality; conveys the general meaning but lacks precision and fluency.\n",
    "\n",
    "30 to 40: Good quality; understandable and relatively accurate with minor errors.\n",
    "\n",
    "40 to 50: Very good quality; accurate and fluent with very few errors.\n",
    "\n",
    "50 and above: Excellent quality; closely resembles human translation.\n",
    "\n",
    "**Interpreting ROUGE scores:** The interpretation of what constitutes a \"good\" ROUGE score can vary depending on several factors, including the specific task, dataset, and domain. Following guidelines indicate a good outcome. \n",
    "\n",
    "ROUGE-1 (unigram overlap): Scores between 0.5 to 0.6 are generally considered good for abstractive summarization tasks.\n",
    "\n",
    "ROUGE-L (Longest Common Subsequence): Scores around 0.4 to 0.5 are often regarded as good, reflecting the model's ability to capture the structure of the reference text.\n",
    "\n",
    "If the score for your translation is unsatisfactory, consider the following questions: \n",
    "\n",
    "#### 1. Is the source audio accurately transcribed? \n",
    "Revisit parameters `glossary_of_transcription_errors` and add text that has incorrect transcriptions.  \n",
    "\n",
    "#### 2. Is the transcribed text free of grammatical errors? \n",
    "Consider using a post-processing step with GPT model to refine the transcription to remove grammatical mistakes and add appropriate punctuations. \n",
    "\n",
    "#### 3. Are there words that make sense to keep in the original language?  \n",
    "There may be new terms or concepts for which a translation in target language may not exist or is not universally understood. Revisit `glossary_of_terms_to_keep_in_original_language` and add such terms."
   ],
   "id": "cbbe2e0cafb384fd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "To recap this guide provides a step-by-step process for translating and dubbing audio from one language to another, making content more accessible to a global audience. The example we used is voice translate an audio file from English to Hindi. The process includes:\n",
    "\n",
    "**1. Transcription**: Using OpenAI's Whisper to transcribe the English audio into text.\n",
    "\n",
    "**2. Translation**: Converting the transcribed English text into Hindi, specifically in the Devanagari script.\n",
    "\n",
    "**3. Text-to-Speech**: Transforming the translated text into spoken Hindi using a text-to-speech service.\n",
    "\n",
    "**4. Benchmarking**: Evaluating the quality and accuracy of the translation with metrics like BLEU or ROUGE and refining the results by adjusting parameters throughout the process.\n",
    "\n",
    "The guide also clarifies the distinction between \"language\" and \"script,\" which are often used interchangeably but have specific meanings critical to the translation task. Language refers to the spoken or written system of communication, while script refers to the characters used to write the language. Understanding this distinction is key to effectively translating and dubbing content. "
   ],
   "id": "38177e0ed7ca3ed6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
